{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![26-Weeks-logo](../images/26-weeks-of-data-science-banner.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 align=\"center\"> Natural Language Processing - Part 2 </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Program so far \n",
    "***\n",
    "\n",
    "* Python\n",
    "* Statistics\n",
    "* Supervised Machine Learning\n",
    "* Unsupervised Machine Learning\n",
    "* NLP Part - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What are we going to learn today ?\n",
    "***\n",
    "\n",
    "\n",
    "- POS Tagging\n",
    "    - Understanding Lexical Units\n",
    "    - Open Class\n",
    "    - Clossed Classes\n",
    "    - Implementation in Python\n",
    "- Chunking\n",
    "- Parsing\n",
    "    - Deep Tree Parsing\n",
    "- Topic Modeling\n",
    "    - What is Topic Modelling\n",
    "\t- Applications O Topic Modelling\n",
    "\t- Latent Dirichlet Allocation\n",
    "\t- How LDA works\n",
    "\t- Topic Modelling with Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We saw last week the bag-of-words approach. As we discussed, bag-of-words fails to capture the structure  of the sentences. Part of Speech helps us overcome this weakness. Let's see how"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### POS Tagging\n",
    "\n",
    "Part of Speech tags are grammatical constituents (Noun, Verbs, Adverb, Adjectives) and this process of POS tagging classify tokens into their part-of-speech tags and label them according to the tagset which is a collection of tags used for the pos tagging. Sounds confusing? Let's make it simpler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Syntax**\t=\thow\twords\tcompose\tto\tform\tlarger\tmeaning\tbearing\tunits\n",
    "\n",
    "\n",
    "**POS**\t=\tsyntactic\tcategories\tfor\twords\n",
    "* You\tcould\tsubstitute\twords\twithin\ta\tclass\tand\thave\ta\tsyntactically\tvalid\tsentence\n",
    "* Gives\tinformation\thow\twords\tcombine\tinto\tlarger\tphrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**POS from School\tgrammar:**\tnoun,\tverb,\tadjective,\tadverb,\tpreposition,\tconjunction,\tpronoun,\tinterjection\n",
    "\n",
    "\n",
    "**Reality**\n",
    "![](../images/img1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Understanding Lexical Units:\n",
    "\n",
    "* There are approximately 8 traditional basic word classes, sometimes called lexical classes or types\n",
    "* These are the ones taught in grade school grammar\n",
    "    - N noun - chair, bandwidth, boy, girl\n",
    "    - V verb - study, debate, munch\n",
    "    - ADJ adjective - purple, tall, ridiculous (includes articles)\n",
    "    - ADV adverb - unfortunately, slowly\n",
    "    - P preposition - of, by, to\n",
    "    - CON conjunction - and, but\n",
    "    - PRO pronoun - I, me, mine\n",
    "    - INT interjection -  um"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Open Class\n",
    "\n",
    "* Can add words to these basic word classes:\n",
    "* Nouns, Verbs, Adjectives, Adverbs.\n",
    "    - Every known human language has nouns and verbs\n",
    "* Nouns: people, places, things\n",
    "    - Classes of nouns\n",
    "* proper vs. common\n",
    "* count vs. mass\n",
    "    - Properties of nouns: can be preceded by a determiner, etc.\n",
    "* Verbs: actions and processes\n",
    "* Adjectives: properties, qualities\n",
    "* Adverbs: hodgepodge!\n",
    "    - Unfortunately, John walked home extremely slowly yesterday\n",
    "* Numerals, ordinals: one, two, three, third, …"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Closed classes\n",
    "\n",
    "* Words are not added to these classes:\n",
    "    * determiners: a, an, the\n",
    "    * pronouns: she, he, I\n",
    "    * prepositions: on, under, over, near, by, …\n",
    "    * over the river and through the woods\n",
    "    * particles: up, down, on, off, …\n",
    "* Used with verbs and have slightly different meaning than when used as a preposition\n",
    "    - she turned the paper over\n",
    "* Closed class words are often function words which have structuring uses in grammar:\n",
    "    - of, it , and , you\n",
    "* Differ more from language to language than open class words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A break-in at the U.S. Justice Department's World Wide Web site last week highlighted the Internet's continued vulnerability to hackers.\n",
      "\n",
      "['A', 'break-in', 'at', 'the', 'U.S.', 'Justice', 'Department', \"'s\", 'World', 'Wide', 'Web', 'site', 'last', 'week', 'highlighted', 'the', 'Internet', \"'s\", 'continued', 'vulnerability', 'to', 'hackers', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "\n",
    "text = open('../data/C50train/AaronPressman/2537newsML.txt').read()\n",
    "sents = nltk.sent_tokenize(text)\n",
    "print(sents[0])\n",
    "print()\n",
    "from nltk import word_tokenize\n",
    "tokens = word_tokenize(sents[0])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'DT'),\n",
       " ('break-in', 'NN'),\n",
       " ('at', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('U.S.', 'NNP'),\n",
       " ('Justice', 'NNP'),\n",
       " ('Department', 'NNP'),\n",
       " (\"'s\", 'POS'),\n",
       " ('World', 'NNP'),\n",
       " ('Wide', 'NNP'),\n",
       " ('Web', 'NNP'),\n",
       " ('site', 'NN'),\n",
       " ('last', 'JJ'),\n",
       " ('week', 'NN'),\n",
       " ('highlighted', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('Internet', 'NNP'),\n",
       " (\"'s\", 'POS'),\n",
       " ('continued', 'JJ'),\n",
       " ('vulnerability', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('hackers', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "These DT, NNP, MD etc are pos tags taken from the standard list of Penn TreeBank Tagsets. It can be found here\n",
    "https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "\n",
    "POS tagging is also supervised learning solution that uses features like previous word, next word, is first letter capitalized? etc.\n",
    "\n",
    "NLTK has a function to get pos tags and it works after tokenization process. \n",
    "\n",
    "In our problem of Author Identification, we can create multiple features using POS Tagging.\n",
    "1. Number of Nouns, Verbs, Adjectives etc.\n",
    "2. How many times sentence starts with Adverb. Meaning words like Basically, Typically etc.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Chunking is a process of extracting phrases(aka chunks) from unstructured text. Instead of just simple tokens which may not represent the actual meaning of text, its advisable to use phrases such as \"New Delhi\" as a single word instead of New and Delhi separate words.\n",
    "\n",
    "- Chunking is done using linguistic rules(language grammar rules), such as when two proper nouns occur together, merge them to make a single word. For Example \"South Africa\".\n",
    "-  Chunking works on top of POS tagging, it uses pos-tags as input and provides chunks as output. \n",
    "-  Similar to POS tags, there are a standard set of Chunk tags like Noun Phrase(NP), Verb Phrase (VP) etc.\n",
    "-  Most data scientist uses N-Grams instead of chunker, but n-grams ends up creating a lot of meaningless words.\n",
    "-  Chunking is very important when you want to extract information from text such as Locations, Person Names etc.\n",
    "-  In Author Identification, we can have features like how many Named entities, the author uses in a sentence.\n",
    "-  What kind of countries/continents, the author mostly refers to in his articles.\n",
    "\n",
    "There are a lot of libraries which gives phrases out-of-box such as Spacy or TextBlob. NLTK just provides a mechanism using regular expressions to generate chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Unidentified/JJ)\n",
      "  hackers/NNS\n",
      "  (VP gained/VBD)\n",
      "  (NP access/NN)\n",
      "  to/TO\n",
      "  (NP the/DT department/NN)\n",
      "  's/POS\n",
      "  (NP web/JJ page/NN)\n",
      "  (P on/IN)\n",
      "  August/NNP\n",
      "  16/CD\n",
      "  and/CC\n",
      "  (VP replaced/VBD)\n",
      "  it/PRP\n",
      "  (PP (P with/IN) (NP a/DT hate-filled/JJ diatribe/NN))\n",
      "  (VP labelled/VBD)\n",
      "  (NP the/DT)\n",
      "  Department/NNP\n",
      "  (P of/IN)\n",
      "  Injustice/NNP\n",
      "  that/WDT\n",
      "  (VP included/VBD)\n",
      "  (NP a/DT swastika/NN)\n",
      "  and/CC\n",
      "  (NP a/DT picture/NN)\n",
      "  (P of/IN)\n",
      "  Adolf/NNP\n",
      "  Hitler/NNP\n",
      "  ./.)\n",
      "(NP Unidentified/JJ)\n",
      "(VP gained/VBD)\n",
      "(NP access/NN)\n",
      "(NP the/DT department/NN)\n",
      "(NP web/JJ page/NN)\n",
      "(P on/IN)\n",
      "(VP replaced/VBD)\n",
      "(PP (P with/IN) (NP a/DT hate-filled/JJ diatribe/NN))\n",
      "(P with/IN)\n",
      "(NP a/DT hate-filled/JJ diatribe/NN)\n",
      "(VP labelled/VBD)\n",
      "(NP the/DT)\n",
      "(P of/IN)\n",
      "(VP included/VBD)\n",
      "(NP a/DT swastika/NN)\n",
      "(NP a/DT picture/NN)\n",
      "(P of/IN)\n"
     ]
    }
   ],
   "source": [
    "#Define your grammar using regular expressions\n",
    "#For example a phrase starting with determiners(The/an/a) followed by noun or adjective will be a noun phrase. such as \"a greedy dog\"\n",
    "parser = ('''\n",
    "    NP: {<DT>? <JJ>* <NN>*} # NP\n",
    "    P: {<IN>}           # Preposition\n",
    "    PP: {<P> <NP>}      # PP -> P NP\n",
    "    VP: {<V.*> <PP|RB|V.*>*}  # VP -> V (NP|PP)*\n",
    "    ''')\n",
    "line=\"Unidentified hackers gained access to the department's web page on August 16 and replaced it with a hate-filled diatribe labelled the Department of Injustice that included a swastika and a picture of Adolf Hitler.\"\n",
    "chunkParser = nltk.RegexpParser(parser)\n",
    "negation_result={}\n",
    "tagged = nltk.pos_tag(nltk.word_tokenize(line))\n",
    "tree = chunkParser.parse(tagged)\n",
    "negated_entity=\"\"\n",
    "negated_value=\"\"\n",
    "negation=None\n",
    "for subtree in tree.subtrees():\n",
    "    print (subtree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Tree Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One of the advanced topics in NLP is the Lexical Analysis of text wherein we try to analyze and understand a text. This process is called deep tree parsing in NLP world where we try to analyze relationships amongst the text.\n",
    "- Text parsing is important when you want to know relationships in text. For example <i>Delhi is capital of India<i>, here Delhi and India are related and having a relationship <b>is capital of<b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP (Det the) (N dog))\n",
      "  (VP\n",
      "    (V saw)\n",
      "    (NP (Det a) (N man) (PP (P in) (NP (Det the) (N park))))))\n",
      "(S\n",
      "  (NP (Det the) (N dog))\n",
      "  (VP\n",
      "    (V saw)\n",
      "    (NP (Det a) (N man))\n",
      "    (PP (P in) (NP (Det the) (N park)))))\n"
     ]
    }
   ],
   "source": [
    "grammar1 = nltk.CFG.fromstring(\"\"\"\n",
    "  S -> NP VP\n",
    "  VP -> V NP | V NP PP\n",
    "  PP -> P NP\n",
    "  V -> \"saw\" | \"ate\" | \"walked\"\n",
    "  NP -> \"John\" | \"Mary\" | \"Bob\" | Det N | Det N PP\n",
    "  Det -> \"a\" | \"an\" | \"the\" | \"my\"\n",
    "  N -> \"man\" | \"dog\" | \"cat\" | \"telescope\" | \"park\"\n",
    "  P -> \"in\" | \"on\" | \"by\" | \"with\"\n",
    "  \"\"\")\n",
    "sent = \"the dog saw a man in the park\"\n",
    "tokens=nltk.word_tokenize(sent)\n",
    "rd_parser = nltk.RecursiveDescentParser(grammar1)\n",
    "for tree in rd_parser.parse(tokens):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](../images/deep_parsing.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here as well we have to define our grammar, which looks quite a tedious job. But there are other NLP packages such as Stanford CoreNLP which provide functions to generate parse tree from unstructured text without defining any grammar.\n",
    "- Parse tree provides us with meaningful and true relations and also kind of relations they share. Also called facts.\n",
    "- Tree Parsing is used to build a knowledge base from the unstructured corpus. Check DbPedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Topic Modelling ?\n",
    "***\n",
    "So far we have visited topics for supervised machine learning for NLP. Now let's see some unsupervised machine learning techniques for NLP.\n",
    "\n",
    "\n",
    "NLP is all about unstructured data, and one of the problems industry is facing today is about the amount of data that any System has to process. Often it's not practical to read through a huge volume of data and get some insights about that data. Consider google news, there are hundreds of thousands of news that get published on daily basis. So we need a way to group news with some keywords in order to understand what is going on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](../images/topic_modelling.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One in red are classes, which are fixed and with the help of training data, we can build news classifier.\n",
    "- But one in green are topics, that are identified run time. And process of identification of topics is totally unsupervised. And Topic modelling is one the best way to understand, repersent any unstructured text without actually getting into it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Topic Modelling__ as the name suggests, it is a process to automatically identify topics present in a text object and to derive hidden patterns exhibited by a text corpus. Thus, assisting better decision making.\n",
    "\n",
    "A __Topic__ can be defined as “a repeating pattern of co-occurring terms in a corpus”. A good topic model should result in – “health”, “doctor”, “patient”, “hospital” for a topic – Healthcare, and “farm”, “crops”, “wheat” for a topic – Farming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications of Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Document Clustering.\n",
    "    1. Group news.\n",
    "    2. Group emails.\n",
    "    3. Group similar medical notes etc.\n",
    "- Keywords Generation. Can be used for SEO.\n",
    "- Build WordCloud.\n",
    "- Build Search Engines.\n",
    "- Build knowledge-graph(aka ontologies).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics are generally important words in text. \n",
    "- Frequency count can be one of the way to identify topics.\n",
    "- TF-IDF can also be used for Topic Modelling.\n",
    "- Or the most famous, LDA (Latent Dirichlet Allocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you have the following set of sentences:\n",
    "\n",
    "- I like to eat broccoli and bananas.\n",
    "- I ate a banana and spinach smoothie for breakfast.\n",
    "- Chinchillas and kittens are cute.\n",
    "- My sister adopted a kitten yesterday.\n",
    "- Look at this cute hamster munching on a piece of broccoli.\n",
    "\n",
    "LDA will try to identify words which have been used in similar context and will calculate probability of occurrence of two words togther.\n",
    "In the above example, LDA will create topics like:\n",
    "    \n",
    "- Topic A: 30% broccoli, 15% bananas, 10% breakfast, 10% munching, … (at which point, you could interpret topic A to be about food)\n",
    "- Topic B: 20% chinchillas, 20% kittens, 20% cute, 15% hamster, … (at which point, you could interpret topic B to be about cute animals).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How LDA Works?\n",
    "\n",
    "LDA involves a detailed understanding of Baysian Probabilistic Approach. However, here is an intuitive explanation of how LDA operates:\n",
    "\n",
    "Take the example given above:\n",
    "\n",
    "* Step 1: We start with tokenizing and then removing stopwords\n",
    "* Step 2: We decide the number of topics, in our case we have decided that number to be 2.\n",
    "* Step 3: Then we take each token in each document and randomly assign it to a topic\n",
    "    * For Statement 1 it might look like this:\n",
    "        * Like: 2\n",
    "        * Eat: 2\n",
    "        * Broccoli: 1\n",
    "        * banana: 2\n",
    "        * Repeat this process for each statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Step 4: Doing this will result in 2 matrices:\n",
    "    1. Document to Topic probability distribution (S -> T)\n",
    "    2. Topic to token probability distribution (T -> W)\n",
    "        * Since these are distributed randomly, they are not accurate\n",
    "        * Hence, we want to modify both matrices to make them as near to the real distribution as possible\n",
    "* Step 5: We again iterate through each token in each document and again assign the topic considering 2 things\n",
    "    * How prevalent is that word across topics? P(word W| Topic T)\n",
    "    * How prevalent are topics in the document? P(Topic T| Document D)\n",
    "    * Let's consider word \"eat\".\n",
    "    * Since it only appears in Topic 2 in statement 1, Hence it is only associated with topic 2.\n",
    "    * Statement 1 is made up of  3/4 of Topic 2 and  1/4 of Topic 1\n",
    "    * This can be interpreted as: word \"eat\" is highly specific to topic 2 and topic 2 makes up of majority of statement 1.\n",
    "    * Hence, eat is more likely to belong to topic 2\n",
    "    * So, we assign \"eat\" to topic 2\n",
    "* Step 6:\n",
    "    * Go to step 4\n",
    "    * We repeat this procedure for each token\n",
    "    * If we perform this entire procedure again and again, we will attain the (S->T) and (T->W) which are approximately equal to actual matrices.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modelling with Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gensim(https://radimrehurek.com/gensim/) package in python implements most of topic modelling algorithms.\n",
    "\n",
    "* We'll walk through a basic application of Topic Modeling with LDA\n",
    "* We'll also cover the basic NLP operations necessary for the application\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's fast-forward through pre-processing\n",
    "\n",
    "* After the processing, we'll have *texts* - a tokenized, stopped and stemmed list of words from a single document\n",
    "* Let’s fast forward and loop through all our documents and appended each one to *texts*\n",
    "* So now *texts* is a list of lists, one list for each of our original documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "from pprint import pprint\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = set(stopwords.words('english'))\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "   \n",
    "# create sample documents\n",
    "# We will use author data\n",
    "import nltk\n",
    "text=open(\"../data/C50train/AaronPressman/2537newsML.txt\").read()\n",
    "sents = nltk.sent_tokenize(text)\n",
    "\n",
    "# compile sample documents into a list\n",
    "doc_set = sents\n",
    "\n",
    "# list for tokenized documents in loop\n",
    "texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### texts\n",
      "[['break', 'u', 'justic', 'depart', 'world', 'wide', 'web', 'site', 'last', 'week', 'highlight', 'internet', 'continu', 'vulner', 'hacker'], ['unidentifi', 'hacker', 'gain', 'access', 'depart', 'web', 'page', 'august', '16', 'replac', 'hate', 'fill', 'diatrib', 'label', 'depart', 'injustic', 'includ', 'swastika', 'pictur', 'adolf', 'hitler'], ['justic', 'offici', 'quickli', 'pull', 'plug', 'vandalis', 'page', 'secur', 'flaw', 'allow', 'hacker', 'gain', 'entri', 'like', 'exist', 'thousand', 'corpor', 'govern', 'web', 'site', 'secur', 'expert', 'said'], ['vast', 'major', 'site', 'vulner', 'said', 'richard', 'power', 'senior', 'analyst', 'comput', 'secur', 'institut'], ['justic', 'depart', 'singl'], ['justic', 'depart', 'offici', 'said', 'compromis', 'web', 'site', 'connect', 'comput', 'contain', 'sensit', 'file'], ['web', 'site', 'http', 'www', 'usdoj', 'gov', 'includ', 'copi', 'press', 'releas', 'speech', 'publicli', 'avail', 'inform'], ['secur', 'breach', 'like', 'graffiti', 'outsid', 'build', 'spokesman', 'bert', 'brandenburg', 'said'], ['organis', 'target', 'past'], ['last', 'year', 'nation', 'islam', 'million', 'man', 'march', 'web', 'site', 'vandalis'], ['hacker', 'make', '250', '000', 'attempt', 'annual', 'break', 'u', 'militari', 'comput', 'accord', 'gener', 'account', 'offic', 'report'], ['window', 'magazin', 'recent', 'found', 'secur', 'flaw', 'web', 'site', 'dozen', 'major', 'corpor'], ['web', 'spectacularli', 'insecur', 'editor', 'mike', 'elgan', 'said'], ['reli', 'secur', 'hole', 'document', 'softwar', 'manufactur', 'month', 'earlier', 'magazin', 'specialist', 'abl', 'gain', 'variou', 'degre', 'unauthoris', 'access', 'differ', 'site'], ['elgan', 'said', 'hacker', 'exploit', 'flaw', 'motiv', 'anger', 'growth', 'commerci', 'internet'], ['common', 'theme', 'hacker', 'fed', 'non', 'hacker', 'internet', 'said'], ['battl', 'complet', 'hopeless'], ['secur', 'web', 'site', 'richard', 'power', 'said'], ['kind', 'measur', 'take'], ['corpor', 'institut', 'take', 'simpli', 'noth', 'bad', 'happen', 'yet'], ['site', 'use', 'multipl', 'layer', 'secur', 'well', 'beyond', 'simpl', 'password', 'protect', 'keep', 'hacker'], ['one', 'site', 'mention', 'window', 'magazin', 'fidel', 'invest'], ['fidel', 'site', 'advertis', 'mutual', 'fund', 'dissemin', 'inform', 'person', 'financ', 'contain', 'confidenti', 'custom', 'inform'], ['fidel', 'offici', 'immedi', 'close', 'loophol', 'identifi', 'magazin', 'spokeswoman', 'said'], ['multipl', 'secur', 'measur', 'previous', 'place', 'would', 'prevent', 'secur', 'breach', 'despit', 'hole', 'spokeswoman', 'ad']]\n",
      "\n",
      "##### The lines in texts\n",
      "['break', 'u', 'justic', 'depart', 'world', 'wide', 'web', 'site', 'last', 'week', 'highlight', 'internet', 'continu', 'vulner', 'hacker']\n",
      "['unidentifi', 'hacker', 'gain', 'access', 'depart', 'web', 'page', 'august', '16', 'replac', 'hate', 'fill', 'diatrib', 'label', 'depart', 'injustic', 'includ', 'swastika', 'pictur', 'adolf', 'hitler']\n",
      "['justic', 'offici', 'quickli', 'pull', 'plug', 'vandalis', 'page', 'secur', 'flaw', 'allow', 'hacker', 'gain', 'entri', 'like', 'exist', 'thousand', 'corpor', 'govern', 'web', 'site', 'secur', 'expert', 'said']\n",
      "['vast', 'major', 'site', 'vulner', 'said', 'richard', 'power', 'senior', 'analyst', 'comput', 'secur', 'institut']\n",
      "['justic', 'depart', 'singl']\n",
      "['justic', 'depart', 'offici', 'said', 'compromis', 'web', 'site', 'connect', 'comput', 'contain', 'sensit', 'file']\n",
      "['web', 'site', 'http', 'www', 'usdoj', 'gov', 'includ', 'copi', 'press', 'releas', 'speech', 'publicli', 'avail', 'inform']\n",
      "['secur', 'breach', 'like', 'graffiti', 'outsid', 'build', 'spokesman', 'bert', 'brandenburg', 'said']\n",
      "['organis', 'target', 'past']\n",
      "['last', 'year', 'nation', 'islam', 'million', 'man', 'march', 'web', 'site', 'vandalis']\n",
      "['hacker', 'make', '250', '000', 'attempt', 'annual', 'break', 'u', 'militari', 'comput', 'accord', 'gener', 'account', 'offic', 'report']\n",
      "['window', 'magazin', 'recent', 'found', 'secur', 'flaw', 'web', 'site', 'dozen', 'major', 'corpor']\n",
      "['web', 'spectacularli', 'insecur', 'editor', 'mike', 'elgan', 'said']\n",
      "['reli', 'secur', 'hole', 'document', 'softwar', 'manufactur', 'month', 'earlier', 'magazin', 'specialist', 'abl', 'gain', 'variou', 'degre', 'unauthoris', 'access', 'differ', 'site']\n",
      "['elgan', 'said', 'hacker', 'exploit', 'flaw', 'motiv', 'anger', 'growth', 'commerci', 'internet']\n",
      "['common', 'theme', 'hacker', 'fed', 'non', 'hacker', 'internet', 'said']\n",
      "['battl', 'complet', 'hopeless']\n",
      "['secur', 'web', 'site', 'richard', 'power', 'said']\n",
      "['kind', 'measur', 'take']\n",
      "['corpor', 'institut', 'take', 'simpli', 'noth', 'bad', 'happen', 'yet']\n",
      "['site', 'use', 'multipl', 'layer', 'secur', 'well', 'beyond', 'simpl', 'password', 'protect', 'keep', 'hacker']\n",
      "['one', 'site', 'mention', 'window', 'magazin', 'fidel', 'invest']\n",
      "['fidel', 'site', 'advertis', 'mutual', 'fund', 'dissemin', 'inform', 'person', 'financ', 'contain', 'confidenti', 'custom', 'inform']\n",
      "['fidel', 'offici', 'immedi', 'close', 'loophol', 'identifi', 'magazin', 'spokeswoman', 'said']\n",
      "['multipl', 'secur', 'measur', 'previous', 'place', 'would', 'prevent', 'secur', 'breach', 'despit', 'hole', 'spokeswoman', 'ad']\n"
     ]
    }
   ],
   "source": [
    "# loop through document list\n",
    "for i in doc_set:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)\n",
    "\n",
    "print(\"\\n##### texts\")\n",
    "print(texts)\n",
    "\n",
    "print(\"\\n##### The lines in texts\")\n",
    "for line in texts:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What's next?\n",
    "\n",
    "* To generate an LDA model, we need to understand how frequently each term occurs within each document\n",
    "* To do that, we need to construct a document-term matrix with a package called *gensim*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topic Modeling with gensim\n",
    "***\n",
    "## Getting started with gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(175 unique tokens: ['break', 'continu', 'depart', 'hacker', 'highlight']...)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The Dictionary() function traverses texts, assigning a unique integer id to each unique token while also collecting word counts and relevant statistics\n",
    "* To see each token’s unique integer id, try -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'break': 0, 'continu': 1, 'depart': 2, 'hacker': 3, 'highlight': 4, 'internet': 5, 'justic': 6, 'last': 7, 'site': 8, 'u': 9, 'vulner': 10, 'web': 11, 'week': 12, 'wide': 13, 'world': 14, '16': 15, 'access': 16, 'adolf': 17, 'august': 18, 'diatrib': 19, 'fill': 20, 'gain': 21, 'hate': 22, 'hitler': 23, 'includ': 24, 'injustic': 25, 'label': 26, 'page': 27, 'pictur': 28, 'replac': 29, 'swastika': 30, 'unidentifi': 31, 'allow': 32, 'corpor': 33, 'entri': 34, 'exist': 35, 'expert': 36, 'flaw': 37, 'govern': 38, 'like': 39, 'offici': 40, 'plug': 41, 'pull': 42, 'quickli': 43, 'said': 44, 'secur': 45, 'thousand': 46, 'vandalis': 47, 'analyst': 48, 'comput': 49, 'institut': 50, 'major': 51, 'power': 52, 'richard': 53, 'senior': 54, 'vast': 55, 'singl': 56, 'compromis': 57, 'connect': 58, 'contain': 59, 'file': 60, 'sensit': 61, 'avail': 62, 'copi': 63, 'gov': 64, 'http': 65, 'inform': 66, 'press': 67, 'publicli': 68, 'releas': 69, 'speech': 70, 'usdoj': 71, 'www': 72, 'bert': 73, 'brandenburg': 74, 'breach': 75, 'build': 76, 'graffiti': 77, 'outsid': 78, 'spokesman': 79, 'organis': 80, 'past': 81, 'target': 82, 'islam': 83, 'man': 84, 'march': 85, 'million': 86, 'nation': 87, 'year': 88, '000': 89, '250': 90, 'accord': 91, 'account': 92, 'annual': 93, 'attempt': 94, 'gener': 95, 'make': 96, 'militari': 97, 'offic': 98, 'report': 99, 'dozen': 100, 'found': 101, 'magazin': 102, 'recent': 103, 'window': 104, 'editor': 105, 'elgan': 106, 'insecur': 107, 'mike': 108, 'spectacularli': 109, 'abl': 110, 'degre': 111, 'differ': 112, 'document': 113, 'earlier': 114, 'hole': 115, 'manufactur': 116, 'month': 117, 'reli': 118, 'softwar': 119, 'specialist': 120, 'unauthoris': 121, 'variou': 122, 'anger': 123, 'commerci': 124, 'exploit': 125, 'growth': 126, 'motiv': 127, 'common': 128, 'fed': 129, 'non': 130, 'theme': 131, 'battl': 132, 'complet': 133, 'hopeless': 134, 'kind': 135, 'measur': 136, 'take': 137, 'bad': 138, 'happen': 139, 'noth': 140, 'simpli': 141, 'yet': 142, 'beyond': 143, 'keep': 144, 'layer': 145, 'multipl': 146, 'password': 147, 'protect': 148, 'simpl': 149, 'use': 150, 'well': 151, 'fidel': 152, 'invest': 153, 'mention': 154, 'one': 155, 'advertis': 156, 'confidenti': 157, 'custom': 158, 'dissemin': 159, 'financ': 160, 'fund': 161, 'mutual': 162, 'person': 163, 'close': 164, 'identifi': 165, 'immedi': 166, 'loophol': 167, 'spokeswoman': 168, 'ad': 169, 'despit': 170, 'place': 171, 'prevent': 172, 'previous': 173, 'would': 174}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Next, our dictionary must be converted into a [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model) -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1)]\n",
      "[(2, 2), (3, 1), (11, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1)]\n",
      "[(3, 1), (6, 1), (8, 1), (11, 1), (21, 1), (27, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 2), (46, 1), (47, 1)]\n",
      "[(8, 1), (10, 1), (44, 1), (45, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1)]\n",
      "[(2, 1), (6, 1), (56, 1)]\n",
      "[(2, 1), (6, 1), (8, 1), (11, 1), (40, 1), (44, 1), (49, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1)]\n",
      "[(8, 1), (11, 1), (24, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1), (68, 1), (69, 1), (70, 1), (71, 1), (72, 1)]\n",
      "[(39, 1), (44, 1), (45, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1)]\n",
      "[(80, 1), (81, 1), (82, 1)]\n",
      "[(7, 1), (8, 1), (11, 1), (47, 1), (83, 1), (84, 1), (85, 1), (86, 1), (87, 1), (88, 1)]\n",
      "[(0, 1), (3, 1), (9, 1), (49, 1), (89, 1), (90, 1), (91, 1), (92, 1), (93, 1), (94, 1), (95, 1), (96, 1), (97, 1), (98, 1), (99, 1)]\n",
      "[(8, 1), (11, 1), (33, 1), (37, 1), (45, 1), (51, 1), (100, 1), (101, 1), (102, 1), (103, 1), (104, 1)]\n",
      "[(11, 1), (44, 1), (105, 1), (106, 1), (107, 1), (108, 1), (109, 1)]\n",
      "[(8, 1), (16, 1), (21, 1), (45, 1), (102, 1), (110, 1), (111, 1), (112, 1), (113, 1), (114, 1), (115, 1), (116, 1), (117, 1), (118, 1), (119, 1), (120, 1), (121, 1), (122, 1)]\n",
      "[(3, 1), (5, 1), (37, 1), (44, 1), (106, 1), (123, 1), (124, 1), (125, 1), (126, 1), (127, 1)]\n",
      "[(3, 2), (5, 1), (44, 1), (128, 1), (129, 1), (130, 1), (131, 1)]\n",
      "[(132, 1), (133, 1), (134, 1)]\n",
      "[(8, 1), (11, 1), (44, 1), (45, 1), (52, 1), (53, 1)]\n",
      "[(135, 1), (136, 1), (137, 1)]\n",
      "[(33, 1), (50, 1), (137, 1), (138, 1), (139, 1), (140, 1), (141, 1), (142, 1)]\n",
      "[(3, 1), (8, 1), (45, 1), (143, 1), (144, 1), (145, 1), (146, 1), (147, 1), (148, 1), (149, 1), (150, 1), (151, 1)]\n",
      "[(8, 1), (102, 1), (104, 1), (152, 1), (153, 1), (154, 1), (155, 1)]\n",
      "[(8, 1), (59, 1), (66, 2), (152, 1), (156, 1), (157, 1), (158, 1), (159, 1), (160, 1), (161, 1), (162, 1), (163, 1)]\n",
      "[(40, 1), (44, 1), (102, 1), (152, 1), (164, 1), (165, 1), (166, 1), (167, 1), (168, 1)]\n",
      "[(45, 2), (75, 1), (115, 1), (136, 1), (146, 1), (168, 1), (169, 1), (170, 1), (171, 1), (172, 1), (173, 1), (174, 1)]\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "for line in corpus:\n",
    "    print(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The doc2bow() function converts dictionary into a bag-of-words\n",
    "* The result, *corpus*, is a list of vectors equal to the number of documents\n",
    "* In each document vector is a series of tuples\n",
    "* The tuples are (term ID, term frequency) pairs\n",
    "* This includes terms that actually occur - terms that do not occur in a document will not appear in that document’s vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Creating the LDA Model\n",
    "\n",
    "*corpus* is a (sparse) document-term matrix and now we’re ready to generate an LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=3, id2word = dictionary, passes=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parameters to the LDA model\n",
    "\n",
    "https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "* num_topics\n",
    "    - required\n",
    "    - An LDA model requires the user to determine how many topics should be generated\n",
    "    - Our document set is small, so we’re only asking for three topics\n",
    "* id2word\n",
    "    - required\n",
    "    - The LdaModel class requires our previous dictionary to map ids to strings\n",
    "* passes\n",
    "    - optional\n",
    "    - The number of laps the model will take through corpus\n",
    "    - The greater the number of passes, the more accurate the model will be\n",
    "    - A lot of passes can be slow on a very large corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LdaModel(num_terms=175, num_topics=3, decay=0.5, chunksize=2000)\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.031*\"secur\" + 0.024*\"depart\" + 0.017*\"magazin\" + 0.017*\"gain\" + 0.017*\"access\" + 0.017*\"hole\" + 0.017*\"spokeswoman\" + 0.017*\"breach\" + 0.017*\"measur\" + 0.017*\"said\"'), (1, '0.033*\"hacker\" + 0.025*\"site\" + 0.018*\"internet\" + 0.018*\"inform\" + 0.018*\"break\" + 0.018*\"u\" + 0.010*\"justic\" + 0.010*\"take\" + 0.010*\"fidel\" + 0.010*\"elgan\"'), (2, '0.049*\"site\" + 0.043*\"web\" + 0.037*\"said\" + 0.031*\"secur\" + 0.019*\"hacker\" + 0.014*\"justic\" + 0.014*\"flaw\" + 0.014*\"offici\" + 0.014*\"corpor\" + 0.014*\"major\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, '0.033*\"hacker\" + 0.025*\"site\" + 0.018*\"internet\" + 0.018*\"inform\" + 0.018*\"break\" + 0.018*\"u\" + 0.010*\"justic\" + 0.010*\"take\" + 0.010*\"fidel\" + 0.010*\"elgan\"')\n",
      "(2, '0.049*\"site\" + 0.043*\"web\" + 0.037*\"said\" + 0.031*\"secur\" + 0.019*\"hacker\" + 0.014*\"justic\" + 0.014*\"flaw\" + 0.014*\"offici\" + 0.014*\"corpor\" + 0.014*\"major\"')\n"
     ]
    }
   ],
   "source": [
    "for topic in ldamodel.print_topics(num_topics=2):\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.031*\"secur\" + 0.024*\"depart\" + 0.017*\"magazin\"')\n",
      "(1, '0.033*\"hacker\" + 0.025*\"site\" + 0.018*\"internet\"')\n",
      "(2, '0.049*\"site\" + 0.043*\"web\" + 0.037*\"said\"')\n"
     ]
    }
   ],
   "source": [
    "for topic in ldamodel.print_topics(num_topics=3, num_words=3):\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Within each topic are the three most probable words to appear in that topic\n",
    "\n",
    "## Topics in detail\n",
    "Let's now look at a topic in detail. Let us see how distinct the topics are, and if they seem to capture any context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.031*\"secur\" + 0.024*\"depart\" + 0.017*\"magazin\" + 0.017*\"gain\" + 0.017*\"access\" + 0.017*\"hole\" + 0.017*\"spokeswoman\" + 0.017*\"breach\" + 0.017*\"measur\" + 0.017*\"said\"\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topic(topicno=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.033*\"hacker\" + 0.025*\"site\" + 0.018*\"internet\" + 0.018*\"inform\" + 0.018*\"break\" + 0.018*\"u\" + 0.010*\"justic\" + 0.010*\"take\" + 0.010*\"fidel\" + 0.010*\"elgan\"\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topic(topicno=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.049*\"site\" + 0.043*\"web\" + 0.037*\"said\" + 0.031*\"secur\" + 0.019*\"hacker\" + 0.014*\"justic\" + 0.014*\"flaw\" + 0.014*\"offici\" + 0.014*\"corpor\" + 0.014*\"major\"\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topic(topicno=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Do the topics make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.031*\"secur\" + 0.024*\"depart\" + 0.017*\"magazin\"')\n",
      "(1, '0.033*\"hacker\" + 0.025*\"site\" + 0.018*\"internet\"')\n",
      "(2, '0.049*\"site\" + 0.043*\"web\" + 0.037*\"said\"')\n"
     ]
    }
   ],
   "source": [
    "for topic in ldamodel.print_topics(num_topics=3, num_words=3):\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Refining the model\n",
    "\n",
    "Two topics seems like a better fit for our documents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.034*\"secur\" + 0.031*\"said\" + 0.030*\"site\" + 0.029*\"hacker\"')\n",
      "(1, '0.029*\"site\" + 0.025*\"web\" + 0.016*\"inform\" + 0.016*\"comput\"')\n"
     ]
    }
   ],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary, passes=20)\n",
    "\n",
    "for topic in ldamodel.print_topics(num_topics=2, num_words=4):\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's try it with more passes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.039*\"said\" + 0.035*\"site\" + 0.031*\"web\" + 0.027*\"hacker\"')\n",
      "(1, '0.023*\"secur\" + 0.023*\"site\" + 0.017*\"inform\" + 0.013*\"depart\"')\n"
     ]
    }
   ],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary, passes=200)\n",
    "\n",
    "for topic in ldamodel.print_topics(num_topics=2, num_words=4):\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Predicting Topic for new documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "doc_set = [\"Are Health professionals justified in saying that brocolli is good for your health?\",\n",
    "           \"Broccoli contains various bioactive compounds that have been shown to reduce inflammation in your body’s tissues\"]\n",
    "\n",
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in doc_set:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)\n",
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus_2 = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus_2, num_topics=3, id2word = dictionary, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LdaModel(num_terms=16, num_topics=3, decay=0.5, chunksize=2000)\n"
     ]
    }
   ],
   "source": [
    "#Lets check by default LDA parameters\n",
    "print(ldamodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.189*\"health\" + 0.108*\"say\" + 0.108*\"good\"')\n",
      "(1, '0.063*\"contain\" + 0.063*\"bodi\" + 0.063*\"variou\"')\n",
      "(2, '0.087*\"shown\" + 0.087*\"tissu\" + 0.087*\"bioactiv\"')\n"
     ]
    }
   ],
   "source": [
    "#Let's print out the topics\n",
    "for topic in ldamodel.print_topics(num_topics=3, num_words=3):\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"After a five year struggle, creditors of the collapsed, fraud-ridden BCCI will receive a payment of $2.65 billion on Tuesday, equal to 24.5 percent of their claims, a spokesman for the liquidators said on Monday.\\nBank of Credit and Commerce International, founded in 1972, was closed by central banks in 1991 and collapsed with debts of more than $12 billion when evidence of massive fraud and money laundering was unearthed leading to a tangled web of litigation which shows no sign of reaching an early conclusion.\\nBCCI had assets of $24 billion and operations in 71 countries at the time of its collapse.\\nLiquidator Deloitte and Touche said a further payment, reportedly of 10 percent, of the admitted claims which total some $10.5 billion should be made in the next 12 to 16 months.\\nThe gross fund of amounts recovered by the liquidators stands at around $4.0 billion and includes $1.5 billion paid by BCCI's majority shareholder, the government of Abu Dhabi, which will pay a further $250 million in due course following a settlement reached earlier this year.\\nThis, in addition to efforts by US authorities which resulted in the recovery of more than $500 million from the United States paved the way for the first dividend payment.\\nA further $245 million was paid by Saudi billionaire Sheikh Khalid bin Mahfouz who the liquidators alleged was involved in covering up the BCCI scandal. Under a 1995 Luxembourg court settlement, Mahfouz agreed to pay without admitting liability, in return for the lawsuits being dropped.\\nThe liquidators still have outstanding claims against the Bank of England, the Institut Monetaire Luxembourgeois (BCCI's operations were based in Luxembourgh) and the auditors of the bank, international accountancy firms Price Waterhouse and Ernst &amp; Whinney, now part of the merged Ernst &amp; Young.\\nPrice Waterhouse has said it is making a multi-billion dollar counter claim against Abu Dhabi.\\nFurther law suits are also pending around the world in an attempt to recover further amounts.\\nThe two biggest groups of creditors of BCCI are in the United Arab Emirates (UAE) and in Britain.\\nThe English liquidators of BCCI, Deloite &amp; Touche, have recovered over $1 billion and have been paid a massive $200 million in fees.\\nIn a report to the High Court earlier this year, the liquidator said legal fees in the liquidation ammounted to over $75 million so far.\\n\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "Text = open('../data/C50train/JoeOrtiz/242939newsML.txt').read()\n",
    "Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/icon/ppt-icons.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br /> \n",
    "\n",
    "##  Mini-Challenge - 1 \n",
    "***\n",
    "In the above data, perform a sentence tokenization on the above data using `sent_tokenize()` and store it in a variable called '**Sent**'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/icon/ppt-icons.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br /> \n",
    "\n",
    "##  Mini-Challenge - 2 \n",
    "***\n",
    "- Iterate over every Sentence in the list **Sent**  using a for loop and convert every sentence into \n",
    "    - lower case \n",
    "    - and then tokenize it using the instantiated object \n",
    "- Now remove the stopwords from the tokens \n",
    "- Lemmatize them using `WordNetLemmatizer().lemmatize()` \n",
    "- Finally append them into the list called **Texts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "en_stop = set(stopwords.words('english'))\n",
    "\n",
    "Lema = WordNetLemmatizer()\n",
    "\n",
    "Texts = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/icon/ppt-icons.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br /> \n",
    "\n",
    "##  Mini-Challenge - 3 \n",
    "***\n",
    "Using the method `.Dictionary()` inside the module `corpora` to create a unique token for every word and also print out the tokens assigned respectively using the `.token2id` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/icon/ppt-icons.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br /> \n",
    "\n",
    "##  Mini-Challenge - 4 \n",
    "***\n",
    "Now convert the dictionary into a bag of words list using the `.doc2bow()` method in `dictionary` and store it in a variable **corpus** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/icon/ppt-icons.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br /> \n",
    "\n",
    "##  Mini-Challenge - 5 \n",
    "***\n",
    "Create an LDA model with number of topics of your choice and your choice of total passes. Now print out the top 5 topics and also the top 3 words in every topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
